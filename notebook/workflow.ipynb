{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow\n",
    "\n",
    "The following notebook contains the step functions workflow definition for training and baseline jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the latest sagemaker, stepfunctions and boto3 SDKs\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install -qU awscli boto3 \"sagemaker>=2.0.0\" # 2.0.0\n",
    "!{sys.executable} -m pip install -qU git+https://github.com/brightsparc/aws-step-functions-data-science-sdk-python.git@sagemaker-v2 \n",
    "!{sys.executable} -m pip show sagemaker stepfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.image_uris import retrieve \n",
    "from sagemaker.processing import Processor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "import stepfunctions\n",
    "from stepfunctions import steps\n",
    "from stepfunctions.inputs import ExecutionInput\n",
    "from stepfunctions.workflow import Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load variables from environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "pipeline_name = os.environ['PIPELINE_NAME']\n",
    "model_name = os.environ['MODEL_NAME']\n",
    "workflow_pipeline_arn = os.environ['WORKFLOW_PIPELINE_ARN']\n",
    "create_experiment_function_name = os.environ['CREATE_EXPERIMENT_LAMBDA']\n",
    "query_training_function_name = os.environ['QUERY_TRAINING_LAMBDA']\n",
    "\n",
    "# Get the session and default bucket\n",
    "session = sagemaker.session.Session()\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "print('region: {}'.format(region))\n",
    "print('role: {}'.format(role))\n",
    "print('pipeline: {}'.format(pipeline_name))\n",
    "print('model name: {}'.format(model_name))\n",
    "print('bucket: {}'.format(bucket))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the input data from the mlops notebook and print values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r input_data \n",
    "input_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the training model output base uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = {\n",
    "    'ModelOutputUri': 's3://{}/{}/model'.format(bucket, model_name), \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Resources\n",
    "\n",
    "### Input Schema\n",
    "\n",
    "Define the input schema for the step functions which can then be used as arguments to resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_input = ExecutionInput(\n",
    "    schema={\n",
    "        \"GitBranch\": str,\n",
    "        \"GitCommitHash\": str,\n",
    "        \"DataVersionId\": str,\n",
    "        \"ExperimentName\": str,\n",
    "        \"TrialName\": str,\n",
    "        \"BaselineJobName\": str,\n",
    "        \"BaselineOutputUri\": str,\n",
    "        \"TrainingJobName\": str\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model monitor baseline\n",
    "\n",
    "Define the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_format = DatasetFormat.csv()\n",
    "env = {\n",
    "    \"dataset_format\": json.dumps(dataset_format),\n",
    "    \"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\",\n",
    "    \"output_path\": \"/opt/ml/processing/output\",\n",
    "    \"publish_cloudwatch_metrics\": \"Disabled\", # Have to be disabled from processing job?\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the processing inputs and outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    ProcessingInput(\n",
    "        source=input_data['BaselineUri'],\n",
    "        destination=\"/opt/ml/processing/input/baseline_dataset_input\",\n",
    "        input_name=\"baseline_dataset_input\",\n",
    "    ),\n",
    "]\n",
    "outputs = [\n",
    "    ProcessingOutput(\n",
    "        source=\"/opt/ml/processing/output\",\n",
    "        destination=execution_input[\"BaselineOutputUri\"],\n",
    "        output_name=\"monitoring_output\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the baseline processing job using the sagemaker [model monitor](https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_model_monitoring.html) container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the default model monitor container\n",
    "region = boto3.Session().region_name\n",
    "monor_monitor_container_uri = retrieve(region=region, framework=\"model-monitor\", version=\"latest\")\n",
    "\n",
    "# Use the base processing where we pass through the \n",
    "monitor_analyzer = Processor(\n",
    "    image_uri=monor_monitor_container_uri,\n",
    "    role=role, \n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=1800,\n",
    "    env=env\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model baseline processing job by running inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#monitor_analyzer.run(inputs=inputs, outputs=outputs, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Training Job\n",
    "\n",
    "Define the training job to run in paralell with the processing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = sagemaker.image_uris.retrieve(region=region, framework=\"xgboost\", version=\"latest\")\n",
    "\n",
    "# Create the estimator\n",
    "xgb = sagemaker.estimator.Estimator(\n",
    "    image_uri,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    output_path=output_data['ModelOutputUri'], # NOTE: Can't use execution_input here\n",
    ")\n",
    "\n",
    "# Set the hyperparameters overriding with any defaults\n",
    "hyperparameters = {\n",
    "    \"max_depth\": \"9\",\n",
    "    \"eta\": \"0.2\",\n",
    "    \"gamma\": \"4\",\n",
    "    \"min_child_weight\": \"300\",\n",
    "    \"subsample\": \"0.8\",\n",
    "    \"objective\": \"reg:linear\",\n",
    "    \"early_stopping_rounds\": \"10\",\n",
    "    \"num_round\": \"3\",\n",
    "}\n",
    "xgb.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "# Specify the data source\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data=input_data['TrainingUri'], content_type=\"csv\")\n",
    "s3_input_val = sagemaker.inputs.TrainingInput(s3_data=input_data['ValidationUri'], content_type=\"csv\")\n",
    "data = {\"train\": s3_input_train, \"validation\": s3_input_val}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the estimator directly in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb.fit(inputs=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Workflow\n",
    "\n",
    "### 1. Create the Experiment\n",
    "\n",
    "Define the create experiment lambda.\n",
    "\n",
    "In future add [ResultsPath](https://docs.aws.amazon.com/step-functions/latest/dg/input-output-resultpath.html) to filter the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_experiment_step = steps.compute.LambdaStep(\n",
    "    'Create Experiment',\n",
    "    parameters={  \n",
    "        \"FunctionName\": create_experiment_function_name,\n",
    "        'Payload': {\n",
    "            \"ExperimentName.$\": '$.ExperimentName',\n",
    "            \"TrialName.$\": '$.TrialName',\n",
    "        }\n",
    "    },\n",
    "    result_path='$.CreateTrialResults'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Run processing Job\n",
    "\n",
    "Define the processing job with a specific failure handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_step = steps.sagemaker.ProcessingStep(\n",
    "    \"Baseline Job\",\n",
    "    processor=monitor_analyzer,\n",
    "    job_name=execution_input[\"BaselineJobName\"],\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    experiment_config={\n",
    "        'ExperimentName': execution_input[\"ExperimentName\"], # '$.ExperimentName', \n",
    "        'TrialName': execution_input[\"TrialName\"],\n",
    "        'TrialComponentDisplayName': \"Baseline\",\n",
    "    },\n",
    "    tags={\n",
    "        \"GitBranch\": execution_input[\"GitBranch\"],\n",
    "        \"GitCommitHash\": execution_input[\"GitCommitHash\"],\n",
    "        \"DataVersionId\": execution_input[\"DataVersionId\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "baseline_step.add_catch(steps.states.Catch(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    next_step=stepfunctions.steps.states.Fail(\n",
    "        \"Baseline failed\", cause=\"SageMakerBaselineJobFailed\"\n",
    "    ),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Run and query training Job\n",
    "\n",
    "Define the training job and add a validation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_step = steps.TrainingStep(\n",
    "    \"Training Job\",\n",
    "    estimator=xgb,\n",
    "    data=data,\n",
    "    job_name=execution_input[\"TrainingJobName\"],\n",
    "    experiment_config={\n",
    "        'ExperimentName': execution_input[\"ExperimentName\"],\n",
    "        'TrialName': execution_input[\"TrialName\"],\n",
    "        'TrialComponentDisplayName': \"Training\",\n",
    "    },\n",
    "    tags={\n",
    "        \"GitBranch\": execution_input[\"GitBranch\"],\n",
    "        \"GitCommitHash\": execution_input[\"GitCommitHash\"],\n",
    "        \"DataVersionId\": execution_input[\"DataVersionId\"],\n",
    "    },\n",
    "    result_path='$.TrainingResults'\n",
    ")\n",
    "\n",
    "training_step.add_catch(stepfunctions.steps.states.Catch(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    next_step=stepfunctions.steps.states.Fail(\n",
    "        \"Training failed\", cause=\"SageMakerTrainingJobFailed\"\n",
    "    ),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model from the training job, note this must follow training to retrieve the expected model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Must follow the training test\n",
    "model_step = steps.sagemaker.ModelStep(\n",
    "    'Save Model',\n",
    "    input_path='$.TrainingResults',\n",
    "    model=training_step.get_expected_model(),\n",
    "    model_name=execution_input['TrainingJobName'],\n",
    "    result_path='$.ModelStepResults'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query training results, and validate that the RMSE error is within an acceptable range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_query_step = steps.compute.LambdaStep(\n",
    "    'Query Training Results',\n",
    "    parameters={  \n",
    "        \"FunctionName\": query_training_function_name,\n",
    "        'Payload':{\n",
    "            \"TrainingJobName.$\": '$.TrainingJobName'\n",
    "        }\n",
    "    },\n",
    "    result_path='$.QueryTrainingResults'\n",
    ")\n",
    "\n",
    "check_accuracy_fail_step = steps.states.Fail(\n",
    "    'Model Error Too Low',\n",
    "    comment='RMSE accuracy higher than threshold'\n",
    ")\n",
    "\n",
    "check_accuracy_succeed_step = steps.states.Succeed('Model Error Acceptable')\n",
    "\n",
    "# TODO: Update query method to query validation error using better result path\n",
    "threshold_rule = steps.choice_rule.ChoiceRule.NumericLessThan(\n",
    "    variable=training_query_step.output()['QueryTrainingResults']['Payload']['results']['TrainingMetrics'][0]['Value'], value=10\n",
    ")\n",
    "\n",
    "check_accuracy_step = steps.states.Choice(\n",
    "    'RMSE < 10'\n",
    ")\n",
    "\n",
    "check_accuracy_step.add_choice(rule=threshold_rule, next_step=check_accuracy_succeed_step)\n",
    "check_accuracy_step.default_choice(next_step=check_accuracy_fail_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add the Error handling in the workflow\n",
    "\n",
    "We will use the [Catch Block](https://aws-step-functions-data-science-sdk.readthedocs.io/en/stable/states.html#stepfunctions.steps.states.Catch) to perform error handling. If the Processing Job Step or Training Step fails, the flow will go into failure state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_jobs = steps.states.Parallel(\"SageMaker Jobs\")\n",
    "sagemaker_jobs.add_branch(baseline_step)\n",
    "sagemaker_jobs.add_branch(steps.states.Chain([training_step, model_step, training_query_step, check_accuracy_step]))\n",
    "\n",
    "# Do we need specific failure for the jobs for group?\n",
    "sagemaker_jobs.add_catch(stepfunctions.steps.states.Catch(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    next_step=stepfunctions.steps.states.Fail(\n",
    "        \"SageMaker Jobs failed\", cause=\"SageMakerJobsFailed\"\n",
    "    ),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Training Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach to the existing workflow\n",
    "workflow = Workflow.attach(workflow_pipeline_arn)\n",
    "workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "workflow_graph = steps.states.Chain([\n",
    "    create_experiment_step,\n",
    "    sagemaker_jobs\n",
    "])\n",
    "\n",
    "workflow.update(workflow_graph)\n",
    "print('Update workflow: {}'.format(workflow.state_machine_arn))\n",
    "\n",
    "time.sleep(3) # Sleep to ensure workflow updated before we continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render the graph of the workflow as defined by the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.render_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect the raw workflow definition and verify the execution variables are correctly passed in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(workflow.definition.to_json(pretty=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(workflow.get_cloudformation_template())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we define the inputs for the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some dummy job and git params\n",
    "job_id = uuid.uuid1().hex\n",
    "git_branch = 'master'\n",
    "git_commit_hash = 'xxx' \n",
    "data_verison_id = 'yyy'\n",
    "\n",
    "# Define the experiment and trial name based on model name and job id\n",
    "experiment_name = \"mlops-{}\".format(model_name)\n",
    "trial_name = \"mlops-{}-{}\".format(model_name, job_id)\n",
    "\n",
    "workflow_inputs = {\n",
    "    \"ExperimentName\": experiment_name,\n",
    "    \"TrialName\": trial_name,\n",
    "    \"GitBranch\": git_branch,\n",
    "    \"GitCommitHash\": git_commit_hash, \n",
    "    \"DataVersionId\": data_verison_id, \n",
    "    \"BaselineJobName\": trial_name, \n",
    "    \"BaselineOutputUri\": f\"s3://{bucket}/{model_name}/monitoring/baseline/mlops-{model_name}-pbl-{job_id}\",\n",
    "    \"TrainingJobName\": trial_name\n",
    "}\n",
    "print(json.dumps(workflow_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then execute the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = workflow.execute(\n",
    "    inputs=workflow_inputs\n",
    ")\n",
    "execution_output = execution.get_output(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render workflow progress with the [render_progress](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Execution.render_progress).\n",
    "\n",
    "This generates a snapshot of the current state of your workflow as it executes. Run the cell again to refresh progress or jump to step functions in the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.render_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [list_events](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Execution.list_events) to list all events in the workflow execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execution.list_events(html=True) # Bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}